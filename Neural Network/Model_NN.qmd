---
title: "NN_for_taxi_fare_Chicago"
author: "Group B2"
format: 
  html: 
    code-link: true
    code-tools: true
    toc: true
    toc-location: right
    df-print: paged
  revealjs:
    output-ext: "Group-B2-PRESENTATION.html"
    toc: false
    code-line-numbers: false
    echo: true
    scrollable: true
    code-link: true
    code-tools: true
    df-print: paged
    slide-number: true
execute: 
  cache: false
  warning: false
---

## Libraries 

```{r}
ensureLibrary <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg)
  }
  library(pkg, character.only = TRUE)
}

ensureLibrary("dplyr")
ensureLibrary("tidyr")
ensureLibrary("purrr")
ensureLibrary("readr")
ensureLibrary("lubridate")
```


## Data load

```{r}

files <- list.files("../all_data", pattern = "^chicago_taxi_\\d{4}_\\d{2}\\.csv$", full.names = TRUE)

taxi_data <- files %>%
  map_dfr(~ read_csv(.x, show_col_types = FALSE))

head(taxi_data)
```
```{r}
nrow(taxi_data)
```


## Filter zero values & outliers

```{r}
# define a function for handling outliers

remove_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower <- Q1 - 1.5 * IQR_val
  upper <- Q3 + 1.5 * IQR_val
  return(x >= lower & x <= upper)
}
```


### Remove NA and zero values

```{r}

df <- taxi_data %>% 
  drop_na(trip_seconds, trip_miles, fare) %>%
  filter(trip_seconds > 0, trip_miles > 0, fare > 1)

rm(taxi_data, files) # free memory
```

### Remove outliers

```{r}
df <- df %>%
  filter(
    remove_outliers(trip_seconds),
    remove_outliers(trip_miles),
    remove_outliers(fare)
  )
nrow(df)
```


## Create Proxy metrics

### [proxy] Average Speed

```{r}
df <- df %>% mutate(
  speed_proxy = (trip_miles / 1.60934) / (trip_seconds/60/60)
)

df <- df %>%
  filter(remove_outliers(speed_proxy))
nrow(df)
```

### [proxy] Change community area flag 

- "unavaliable" if one of values is NA (i.e. we cannot define if the area was changed during trip)
- "changed" if the community area of destination differs to the community area of strarting position
- "single-area" if not differs

```{r}
df <- df %>%
  mutate(
    change_ca_flag = case_when(
      is.na(pickup_community_area) ~ "unavailable",
      is.na(dropoff_community_area) ~ "unavailable",
      pickup_community_area == dropoff_community_area ~ "single-area",
      pickup_community_area != dropoff_community_area ~ "changed"
    ),
    change_ca_flag = factor(change_ca_flag)
  ) 
```

### Day part & weekday

```{r}
df <- df %>%
  mutate(
    hour = hour(trip_start_timestamp),
    day_part = case_when(
      hour >= 0 & hour < 6   ~ "night",
      hour >= 6 & hour < 12  ~ "morning",
      hour >= 12 & hour < 18 ~ "midday",
      hour >= 18 & hour <= 23 ~ "evening"
    ),
    day_part = factor(
      day_part,
      levels = c("night", "morning", "midday", "evening"),
      ordered = TRUE
    ),

    weekday = case_when(
      wday(trip_start_timestamp, week_start = 1) == 1 ~ "Mon",
      wday(trip_start_timestamp, week_start = 1) == 2 ~ "Tue",
      wday(trip_start_timestamp, week_start = 1) == 3 ~ "Wed",
      wday(trip_start_timestamp, week_start = 1) == 4 ~ "Thu",
      wday(trip_start_timestamp, week_start = 1) == 5 ~ "Fri",
      wday(trip_start_timestamp, week_start = 1) == 6 ~ "Sat",
      wday(trip_start_timestamp, week_start = 1) == 7 ~ "Sun"
    ),
    weekday = factor(
      weekday,
      levels = c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
      ordered = TRUE
    ),
    month = month(trip_start_timestamp, label = TRUE, abbr = TRUE)
  ) 
```


## Data summary

```{r}
glimpse(df)
```

# Model

```{r}
library(caret)
library(neuralnet)
library(fastDummies)
```

## Function for evaluation

```{r}
evaluate_model <- function(y, duration_seconds, prediction) {
  if (length(y) != length(prediction) || length(y) != length(duration_seconds)) {
    stop("All input vectors must have the same length.")
  }

  # Convert duration from seconds to minutes
  duration_minutes <- duration_seconds / 60

  # Metric 1: RMSE scaled to 10 minutes,
  # Compute squared errors,
  squared_errors <- (y - prediction)^2

  # Scale errors per 10 minutes,
  scaling_factors <- 10 / duration_minutes
  rmse_10min <- sqrt(mean(squared_errors * scaling_factors^2, na.rm = TRUE))

  # Metric 2: Mean Absolute Error as percentage of true value,
  ae_percent <- abs(y - prediction) / y
  mean_ae_percent <- mean(ae_percent, na.rm = TRUE)

  # Return results,
  list(
    RMSE = sqrt(mean(squared_errors, na.rm = TRUE)),
    RMSE_10min = rmse_10min,
    RMSE_10min_OK = rmse_10min < 2,
    Mean_AE_percent = mean_ae_percent,
    Mean_AE_percent_OK = mean_ae_percent < 0.05
  )
}
```


### feature engineering

```{r}
set.seed(123) 
df_usefull <- df[sample(nrow(df), 10000), ] %>%
  select(
    fare,
    trip_seconds, 
    trip_miles, 
    payment_type, 
    speed_proxy, 
    change_ca_flag,
    hour, day_part, weekday, month
  ) %>%
  na.omit()

nrow(df_usefull)

```


```{r}
rm(df) # free memory

df_dummies <- dummy_cols(df_usefull,
                         select_columns = c("day_part", 
                                            "weekday", 
                                            "month", 
                                            "payment_type", 
                                            "company", 
                                            "change_ca_flag"),
                         remove_first_dummy = TRUE,
                         remove_selected_columns = TRUE)

names(df_dummies) <- make.names(names(df_dummies), unique = TRUE)

glimpse(df_dummies)
```

## Neural Net Models

```{r}
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

numeric_cols <- sapply(df_dummies, is.numeric)
df_norm <- as.data.frame(lapply(df_dummies[, numeric_cols], normalize))

set.seed(123)
index <- sample(1:nrow(df_norm), 0.7 * nrow(df_norm))
train <- df_norm[index, ]
test <- df_norm[-index, ]

predictors <- setdiff(names(train), "fare")

formula_nn <- as.formula(paste("fare ~", paste(predictors, collapse = " + ")))
```

### Initial Neural Network

```{r}
start_time <- Sys.time()
set.seed(123)
nn <- neuralnet(formula_nn,
                data = train,
                hidden = c(12, 5),
                linear.output = TRUE,
                stepmax = 1e6)
end_time <- Sys.time()
training_time <- end_time - start_time
```


```{r}
plot(nn)
```


```{r}
predicted <- compute(nn, test[, predictors])$net.result

fare_min <- min(df_dummies$fare)
fare_max <- max(df_dummies$fare)
predicted_rescaled <- predicted * (fare_max - fare_min) + fare_min

duration <- df_usefull$trip_seconds[-index]

true_values <- df_usefull$fare[-index]

evaluate_model(true_values, duration, predicted_rescaled)
```
```{r}
validation_table <- data.frame(
  training_time = integer(0),
  seed = integer(0),
  hidden_levels = integer(0),
  l1_neurons = integer(0),
  l2_neurons = integer(0),
  RMSE = numeric(0),
  RMSE_10min = numeric(0),
  RMSE_10min_OK = logical(0),
  Mean_AE_percent = numeric(0),
  Mean_AE_percent_OK = logical(0)
)
```

```{r}
validation_table[nrow(validation_table) + 1, ] <- c(as.numeric(difftime(end_time, start_time, units = "secs"))
  ,123, 2 ,12,5, 
  evaluate_model(true_values, duration, predicted_rescaled))

validation_table
```

### Test the model with different neuron number


```{r}

possible_neurons <- c(5:8, seq(9,18,3))
possible_neurons
```


```{r}
for (n1 in possible_neurons) {
  for (n2 in possible_neurons) {
    if (n1 < n2) {
      next # Skip if n1 is less than n2 
    }
  start_time <- Sys.time() 
  set.seed(1111)
  nn_loop <- neuralnet(formula_nn,
                data = train,
                hidden = c(n1, n2),
                linear.output = TRUE,
                stepmax = 1e6)

  end_time <- Sys.time()
  training_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
  
  predicted <- compute(nn_loop, test[, predictors])$net.result
  
  predicted_rescaled <- predicted * (fare_max - fare_min) + fare_min
  
  validation_table[nrow(validation_table) + 1, ] <- c(training_time,1111, 2 ,n1,n2, 
                                                      evaluate_model(
                                                        true_values, 
                                                        duration, 
                                                        predicted_rescaled))
  if (n1 %in% c(5,8,12,18) & n2 ==5) {
    print(validation_table)
  }
  
  }
}

```

```{r}
knitr::kable(validation_table)

```

As we can see, not even one model align within the RMSE_10min < 2 and Mean_AE_percent < 0.05 criteria.

```{r}

validation_table %>%
  arrange(RMSE_10min) %>%
  head(3)
```

The best model in respect to RMSE_10min is the (12,5) neurons on hidden levels respectively. 

```{r}

validation_table %>%
  arrange(Mean_AE_percent) %>%
  head(3)


```

The best model in respect to Mean_AE_percent is the (5,5) neurons on hidden levels respectively. 


