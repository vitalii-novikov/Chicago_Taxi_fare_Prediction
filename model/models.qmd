---
title: "Model"
author: "Nicolas Pozdena BSc."
format: pdf
editor: visual
---

```{r echo: false}

ensureLibrary <- function(pkg) {
  suppressPackageStartupMessages({
    if (!requireNamespace(pkg, quietly = TRUE)) {
      install.packages(pkg)
    }
    library(pkg, character.only = TRUE)
  })
}

ensureLibrary("httr")
ensureLibrary("jsonlite")
ensureLibrary("dplyr")
ensureLibrary("tidyr")
ensureLibrary("lubridate")
ensureLibrary("ggplot2")
ensureLibrary("readr")
ensureLibrary("caret")
ensureLibrary("purrr")
ensureLibrary("ggplot2")
ensureLibrary("neuralnet")
ensureLibrary("xgboost")
ensureLibrary("randomForest")
ensureLibrary("patchwork")
ensureLibrary("Metrics")

```

```{r echo: false}
load_all_csv <- function(folder_path) {
  
 files <- list.files(path = folder_path, pattern = "\\.csv$", full.names = TRUE)

  if (length(files) == 0) {
    warning("No CSV files found in the folder.")
    return(NULL)
  }

  all_data <- list()
  
  for (f in files) {
    temp <- read.csv(f)
    temp <- temp%>% select(-X, -trip_start_timestamp, -pickup_census_tract, -dropoff_census_tract, -pickup_census_tract_centroid, -dropoff_census_tract_centroid, -type, -dropoff_census_tract.1, -pickup_census_tract.1) %>% mutate(weekday = as.factor(weekday))
    all_data[[f]] <- temp  # use filename as list name (optional but useful)
  }

  return(all_data)
}

evaluate_and_plot_full_model <- function(y, duration_seconds, prediction, model = "Model", dataset = "Dataset") {
  if (length(y) != length(prediction) || length(y) != length(duration_seconds)) {
    stop("All input vectors must have the same length.")
  }

  # Convert inputs
  y <- as.numeric(y)
  prediction <- as.numeric(prediction)
  duration_minutes <- duration_seconds / 60

  # --- Metrics ---
  rmse_val <- rmse(y, prediction)
  mae_val <- mae(y, prediction)
  r2_val <- 1 - sum((y - prediction)^2) / sum((y - mean(y))^2)

  squared_errors <- (y - prediction)^2
  scaling_factors <- 10 / duration_minutes
  rmse_10min <- sqrt(mean(squared_errors * scaling_factors^2, na.rm = TRUE))

  ae_percent <- abs(y - prediction) / y
  mean_ae_percent <- mean(ae_percent, na.rm = TRUE)

  is_ok <- (rmse_10min < 2) & (mean_ae_percent < 0.05)

  # --- Plots ---

  # 1. Prediction vs Actual
  p_pred <- ggplot(data.frame(y, prediction), aes(x = y, y = prediction)) +
    geom_point(color = "steelblue", alpha = 0.6) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(
      title = paste(model, "Predictions"),
      subtitle = paste0("Dataset: ", dataset, " (N = ", length(prediction), ")"),
      x = "Actual",
      y = "Predicted"
    ) +
    theme_minimal()

  # 2. Metric bar chart
  metric_df <- data.frame(
    Metric = c("RMSE", "MAE", "RÂ²"),
    Value = c(rmse_val, mae_val, r2_val)
  )

  p_metrics <- ggplot(metric_df, aes(x = Metric, y = Value, fill = Metric)) +
    geom_col(width = 0.6, show.legend = FALSE) +
    geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 4) +
    labs(
      title = "Model Evaluation Metrics",
      y = NULL, x = NULL
    ) +
    theme_minimal()

  # 3. Summary: RMSE 10min vs %AE
  summary_df <- data.frame(
    RMSE_10min = rmse_10min,
    Mean_AE_percent = mean_ae_percent,
    Status = ifelse(is_ok, "OK", "Not OK")
  )

  p_summary <- ggplot(summary_df, aes(x = RMSE_10min, y = Mean_AE_percent)) +
    geom_point(aes(color = Status), size = 5) +
    geom_vline(xintercept = 2, linetype = "dashed", color = "black") +
    geom_hline(yintercept = 0.05, linetype = "dashed", color = "black") +
    scale_color_manual(values = c("OK" = "green", "Not OK" = "red")) +
    labs(
      title = "Evaluation: Scaled RMSE vs % Error",
      x = "RMSE (scaled to 10 min)",
      y = "Mean Absolute Error (%)"
    ) +
    theme_minimal()

  # 4. Individual error scatter
  individual_df <- data.frame(
    AbsoluteError_10min = abs(y - prediction) * (10 / duration_minutes),
    PercentError = ae_percent
  )

  p_individual <- ggplot(individual_df, aes(x = AbsoluteError_10min, y = PercentError)) +
    geom_point(alpha = 0.6, color = "blue") +
    geom_hline(yintercept = 0.05, linetype = "dashed", color = "black") +
    labs(
      title = "Individual Errors",
      x = "Absolute Error (scaled to 10 min)",
      y = "Percentage Error"
    ) +
    theme_minimal()

  # Return all 4 plots as a named list
  return(list(
    scatter_plot = p_pred,
    metric_bar = p_metrics,
    summary_plot = p_summary,
    individual_errors_plot = p_individual
  ))
}
```

# Modelling

## loading dataset



```{}
```

## splits

```{r}
set.seed(111)

size = nrow(reducedDf)
wholeData <- sample(1:size, 0.01 * size)
mD <- reducedDf[wholeData, ]


workOnData <- function(modelData){
  
  modelData$weekday <- as.factor(weekdays(modelData$trip_start_timestamp))
  modelData$day <- as.POSIXlt(modelData$trip_start_timestamp)$mday
  modelData$month <- as.POSIXlt(modelData$trip_start_timestamp)$mon + 1
  modelData$year <- as.POSIXlt(modelData$trip_start_timestamp)$year + 1900
  
  ts_lt <- as.POSIXlt(modelData$trip_start_timestamp)
  
  # Extract hours, minutes, seconds
  hour <- ts_lt$hour
  minute <- ts_lt$min
  second <- ts_lt$sec
  
  
  time_decimal <- hour + minute / 60 + second / 3600
  
  
  modelData$time <- time_decimal
  
  
  modelData <- modelData %>% select(-trip_start_timestamp)
  
  modelData %>% head()
  
  rm(hour)
  rm(minute)
  rm(second)
  rm(ts_lt)
  rm(time_decimal)
  
  return(modelData)
}

mD <- workOnData(mD) 

```

```{r}

train_index <- sample(1:nrow(mD), 0.8 * nrow(mD))
train <- mD[train_index, ]
test <- mD[-train_index, ]

```

```{r}
form <- fare ~ .
```

### Neural Net

```{r}


train_matrix <- model.matrix(form, data = train)[, -1]
test_matrix  <- model.matrix(form, data = test)[, -1]


train_data_nn <- data.frame(fare = train$fare, train_matrix)
test_data_nn  <- data.frame(fare = test$fare, test_matrix)


nn_formula <- as.formula(paste("fare ~", paste(colnames(train_matrix), collapse = " + ")))


model_nn <- neuralnet(nn_formula, data = train_data_nn, hidden = c(2, 3),linear.output = FALSE)
pred_nn <- compute(model_nn, test_matrix)$net.result

showPredictions(pred_nn, test$fare, "Neural Net")
```

### GLM

```{r}
model_glm <- glm(form, data=train)
pred_glm <- predict(model_glm, newdata=test)

showPredictions(pred_glm, test$fare, "GLM")
```

### XGBoost

```{r}
x_train <- model.matrix(form, data = train)[, -1]
x_test  <- model.matrix(form, data = test)[, -1]
y_train <- train$fare
y_test  <- test$fare

model_xgb <- xgboost(data = x_train,label = y_train,nrounds = 50, objective = "reg:squarederror", verbose = 0)
pred_xgb <- predict(model_xgb, newdata = x_test)

showPredictions(pred_xgb, y_test, "XGBoost")

```

### Random Forest

```{r}
model_rf <- randomForest(form, data=train)
pred_rf <- predict(model_rf, newdata=test)

showPredictions(pred_rf, test$fare, "Random Forrest")
```

### LM

```{r}
model_lm <- lm(form, data=train)
pred_lm <- predict(model_lm, newdata=test)

showPredictions(pred_lm, test$fare, "Linear Model")

```

## percentage test

```{r}
set.seed(123)
target_col <- "fare"
form <- as.formula(paste(target_col, "~ ."))
percentages <- c(0.001, 0.002, 0.003, 0.004, 0.005)


results <- data.frame()

# Loop through each percentage
for (p in percentages) {
  print(paste("Running", p*100, "%"))
  
  n_rows <- ceiling(nrow(reducedDf) * p)
  df_sample <- reducedDf[sample(nrow(reducedDf), n_rows), ]
  
  df_sample <- workOnData(df_sample)

  train_idx <- sample(1:nrow(df_sample), size = 0.8 * nrow(df_sample))
  train <- df_sample[train_idx, ]
  test  <- df_sample[-train_idx, ]

  y_test <- test[[target_col]]
  
  # ---- MODELS ----

  model_list <- list()

  # 1. LM
  start_time <- Sys.time()
  model <- lm(form, data = train)
  elapsed <- Sys.time() - start_time
  preds <- predict(model, newdata = test)
  model_list[["LM"]] <- list(time = elapsed, pred = preds)

  # 2. GLM
  start_time <- Sys.time()
  model <- glm(form, data = train)
  elapsed <- Sys.time() - start_time
  preds <- predict(model, newdata = test)
  model_list[["GLM"]] <- list(time = elapsed, pred = preds)

  # 3. Random Forest
  start_time <- Sys.time()
  model <- randomForest(form, data = train)
  elapsed <- Sys.time() - start_time
  preds <- predict(model, newdata = test)
  model_list[["RF"]] <- list(time = elapsed, pred = preds)

  # 4. XGBoost
  x_train <- model.matrix(form, data = train)[, -1]
  x_test  <- model.matrix(form, data = test)[, -1]
  y_train <- train[[target_col]]
  
  start_time <- Sys.time()
  model <- xgboost(data = x_train, label = y_train, nrounds = 20, objective = "reg:squarederror", verbose = 0)
  elapsed <- Sys.time() - start_time
  preds <- predict(model, newdata = x_test)
  model_list[["XGB"]] <- list(time = elapsed, pred = preds)


  # ---- Collect results ----
  for (model_name in names(model_list)) {
    entry <- model_list[[model_name]]
    preds <- entry$pred
    elapsed <- entry$time
    
    rmse_val <- rmse(y_test, preds)
    mae_val  <- mae(y_test, preds)
    r2_val   <- 1 - sum((y_test - preds)^2) / sum((y_test - mean(y_test))^2)

    results <- rbind(results, data.frame(
      percentage = p,
      rows = n_rows,
      model = model_name,
      time = as.numeric(elapsed, units = "secs"),
      rmse = rmse_val,
      mae = mae_val,
      r2 = r2_val
    ))
  }
}

# Show results
print(results)
```

```{r}

results %>%
  ggplot(aes(x = as.factor(rows), y = rmse, color = model, size=time)) +
  geom_point() +
  labs(
    title = "RMSE by Model and Data Percentage",
    x = "ROWS",
    y = "RMSE",
    fill = "Model"
  ) +
  theme_minimal()

```

```{r}

metric_deltas_scaled <- results %>% filter(rows != 20827) %>%
  arrange(model, rows) %>%
  group_by(model) %>%
  mutate(
    delta_rows = rows - lag(rows),
    delta_rmse = rmse - lag(rmse),
    delta_mae  = mae  - lag(mae),
    delta_r2   = r2   - lag(r2),
    delta_time = time - lag(time)
  ) %>%
  filter(!is.na(delta_rows) & delta_rows > 0) %>%
  summarise(
    rmse_per_10k = mean(delta_rmse / delta_rows * 10000, na.rm = TRUE),
    mae_per_10k  = mean(delta_mae / delta_rows * 10000, na.rm = TRUE),
    r2_per_10k   = mean(delta_r2  / delta_rows * 10000, na.rm = TRUE),
    time_per_10k = mean(delta_time/ delta_rows * 10000, na.rm = TRUE)
  )
metric_deltas_scaled
```


```{r}

remove_outliers_z <- function(df, feature1, feature2, threshold = 3) {
  z1 <- abs(scale(df[[feature1]]))
  z2 <- abs(scale(df[[feature2]]))
  
  df_clean <- df[(z1 < threshold) & (z2 < threshold), ]
  return(df_clean)
}

```


```{r}
df <- load_all_csv("../all_data/datasets")
form <- fare ~ .
percentage <- 0.8
```


```{r}
models <- list(
  "GLM" = glm,
  "LM" = lm,
  "RandomForest" = randomForest
)

for (ds in rev(names(df))) {
    dataSet <- df[[ds]]
    dataSet <- remove_outliers_z(dataSet, "fare", "trip_seconds", 2)
    train_index <- sample(1:nrow(dataSet), percentage * nrow(dataSet))
    train <- dataSet[train_index, ]
    test <- dataSet[-train_index, ]
    
     # Extract target and clean test set
    y <- test$fare
    test <- test %>% select(-fare)
    
  for (m in names(models)) {
    print(paste0("Working on ", m , " with ", ds))
    
    
   
    
    # Fit model
    model_func <- models[[m]]
    model <- model_func(form, data = train, na.action = na.omit)
    
    # Predict
    pred <- predict(model, newdata = test)
    
    # Plot evaluation
    print(evaluate_and_plot_full_model(y, test$trip_seconds, pred, model = m, dataset = ds))
  }
}
```
